{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "import os\n",
    "from mylib.models import vae\n",
    "from mylib.data.data_loader.dataloader import DataLoader_noise\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets.mnist import FashionMNIST\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from mylib import models\n",
    "from mylib.utils import AverageMeter, ProgressMeter, fix_seed, accuracy, save_checkpoint\n",
    "import types\n",
    "import numpy as np\n",
    "# --- parsing and configuration --- #\n",
    "from collections import OrderedDict, defaultdict\n",
    "import mylib.models as models\n",
    "from mylib.utils import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoisy-labels\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/ubuntu/anaconda3/envs/idnl/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/ubuntu/Storage/Noisy_Labels/start/IDLN/wandb/run-20220329_212155-2omstgbl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/noisy-labels/DNL_new/runs/2omstgbl\" target=\"_blank\">super-sun-9</a></strong> to <a href=\"https://wandb.ai/noisy-labels/DNL_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/noisy-labels/DNL_new/runs/2omstgbl?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f92dc9c16d8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"DNL_new\", entity=\"noisy-labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from mylib.utils import AverageMeter, ProgressMeter, fix_seed, accuracy, adjust_learning_rate, save_checkpoint\n",
    "from mylib.data.data_loader import load_noisydata\n",
    "from causalNL import run_vae\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_args = types.SimpleNamespace()\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=vae_args.alpha_plan[epoch]\n",
    "        param_group['betas']=(vae_args.beta1_plan[epoch], 0.999) # Only change beta1\n",
    "        \n",
    "        \n",
    "\n",
    "def log_standard_categorical(p, reduction=\"mean\"):\n",
    "    \"\"\"\n",
    "    Calculates the cross entropy between a (one-hot) categorical vector\n",
    "    and a standard (uniform) categorical distribution.\n",
    "    :param p: one-hot categorical distribution\n",
    "    :return: H(p, u)\n",
    "    \"\"\"\n",
    "    # Uniform prior over y\n",
    "    prior = F.softmax(torch.ones_like(p), dim=1)\n",
    "    prior.requires_grad = False\n",
    "\n",
    "    cross_entropy = -torch.sum(p * torch.log(prior + 1e-8), dim=1)\n",
    "    # print(cross_entropy)\n",
    "  \n",
    "    if reduction==\"mean\":\n",
    "        cross_entropy = torch.mean(cross_entropy)\n",
    "    else:\n",
    "        cross_entropy = torch.sum(cross_entropy)\n",
    "    \n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "\n",
    "def loss_coteaching(y_1, y_2, t, forget_rate):\n",
    "    loss_1 = F.cross_entropy(y_1, t, reduce = False)\n",
    "    ind_1_sorted = np.argsort(loss_1.cpu().data).cuda()\n",
    "    loss_1_sorted = loss_1[ind_1_sorted]\n",
    "\n",
    "    loss_2 = F.cross_entropy(y_2, t, reduce = False)\n",
    "    ind_2_sorted = np.argsort(loss_2.cpu().data).cuda()\n",
    "    loss_2_sorted = loss_2[ind_2_sorted]\n",
    "\n",
    "    remember_rate = 1 - forget_rate\n",
    "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
    "\n",
    "    ind_1_update=ind_1_sorted[:num_remember]\n",
    "    ind_2_update=ind_2_sorted[:num_remember]\n",
    "    # exchange\n",
    "    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update])\n",
    "    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update])\n",
    "\n",
    "    return torch.sum(loss_1_update)/num_remember, torch.sum(loss_2_update)/num_remember\n",
    "\n",
    "\n",
    "\n",
    "def vae_loss(x_hat, data, n_logits, targets, mu, log_var, c_logits, h_c_label):\n",
    "    # x loss \n",
    "    l1 = 0.1*F.mse_loss(x_hat, data, reduction=\"mean\")\n",
    "\n",
    "    # \\tilde{y]} loss\n",
    "    l2 = 0.1*F.cross_entropy(n_logits, targets, reduction=\"mean\")\n",
    "    #  uniform loss for x\n",
    "    l3 = -0.00001*log_standard_categorical(h_c_label, reduction=\"mean\")\n",
    "    #  Gaussian loss for z\n",
    "    l4 = -0.01 *torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return (l1+l2+l3+l4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- train and test --- #\n",
    "def train(epoch, model, train_loader, optimizers, device):\n",
    "\n",
    "    n_top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    co1_loss = AverageMeter('Acc@1', ':6.2f')\n",
    "    co2_loss = AverageMeter('Acc@1', ':6.2f')\n",
    "    vae1_loss = AverageMeter('Acc@1', ':6.2f')\n",
    "    vae2_loss = AverageMeter('Acc@1', ':6.2f')\n",
    "    vae_model1 = model[\"vae_model1\"].train()\n",
    "    vae_model2 = model[\"vae_model2\"].train()\n",
    "    optimizer1 = optimizers[\"vae1\"]\n",
    "\n",
    "    optimizer2 = optimizers[\"vae2\"]\n",
    "\n",
    "    for _, (data, targets, _, _, _) in enumerate(train_loader):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "     \n",
    "        #forward\n",
    "        x_hat1, n_logits1, mu1, log_var1, c_logits1, y_hat1  = vae_model1(data)\n",
    "        x_hat1, n_logits1, mu1, log_var1, c_logits1, y_hat1 = x_hat1.cuda(), n_logits1.cuda(), mu1.cuda(), log_var1.cuda(), c_logits1.cuda(), y_hat1.cuda()\n",
    "        x_hat2, n_logits2, mu2, log_var2, c_logits2, y_hat2 = vae_model2(data)\n",
    "        x_hat2, n_logits2, mu2, log_var2, c_logits2, y_hat2= x_hat2.cuda(), n_logits2.cuda(), mu2.cuda(), log_var2.cuda(), c_logits2.cuda(), y_hat2.cuda()\n",
    "        #calculate acc\n",
    "        n_acc1, _ = accuracy(n_logits1, targets, topk=(1, 2))\n",
    "\n",
    "        n_top1.update(n_acc1.item(), data.size(0))\n",
    "\n",
    " \n",
    "        # calculate loss\n",
    "        vae_loss_1 = vae_loss(x_hat1, data, n_logits1, targets, mu1, log_var1, c_logits1, y_hat1)\n",
    "        vae_loss_2 = vae_loss(x_hat2, data, n_logits2, targets, mu2, log_var2, c_logits2, y_hat2)\n",
    "\n",
    "        co_loss_1, co_loss_2 = loss_coteaching(c_logits1, c_logits2, targets, vae_args.rate_schedule[epoch])\n",
    "\n",
    "        loss_1 =  co_loss_1+vae_loss_1\n",
    "        loss_2 =   co_loss_2+vae_loss_2\n",
    "        co1_loss.update(co_loss_1.item(), data.size(0))\n",
    "        co2_loss.update(co_loss_2.item(), data.size(0))\n",
    "        vae1_loss.update(vae_loss_1.item(), data.size(0))\n",
    "        vae2_loss.update(vae_loss_2.item(), data.size(0))\n",
    "        optimizer1.zero_grad()\n",
    "        loss_1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.zero_grad()\n",
    "        loss_2.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "    wandb.log({\n",
    "        \"Co-teaching_1 loss\": co1_loss.avg,\n",
    "        \"Co-teaching_2 loss\": co2_loss.avg,\n",
    "        \"VAE_1 Loss\":vae1_loss.avg,\n",
    "        \"VAE_1 Loss\":vae2_loss.avg,\n",
    "        \"Accuracy Train Noisy\":n_top1.avg,\n",
    "        \"Total_1 (co1 + vae1)\": co1_loss.avg+vae1_loss.avg,\n",
    "        \"Total_2 (co2 + vae2)\": co1_loss.avg+vae1_loss.avg})\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.5f}/{:.5f}/{:.5f}/{:.5f}'.format( epoch,co1_loss.avg, co2_loss.avg,vae1_loss.avg,vae2_loss.avg))\n",
    "    print('====> train noisy acc: {:.4f}'.format(n_top1.avg))\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, test_loader, device, dataset):\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    vae_model1 = model[\"vae_model1\"].eval()\n",
    "    vae_model1 = model[\"vae_model1\"].eval()\n",
    "    new_labels  = []\n",
    "    recon_points = []\n",
    "    example_images = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _, clean_targets, _, _)  in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            clean_targets = clean_targets.to(device)\n",
    "            x_hat, _, _, _, c_logits,_ = vae_model1(data)\n",
    " \n",
    "            # calculate the training acc\n",
    "            h_c_acc1, _ = accuracy(c_logits, clean_targets, topk=(1, 2))\n",
    "            top1.update(h_c_acc1.item(), data.size(0))\n",
    "    \n",
    "            max_probs, target_u = torch.max(c_logits, dim=-1)\n",
    "            recon_points += x_hat.tolist()\n",
    "            new_labels +=target_u.tolist()\n",
    "\n",
    "            example_images.append(wandb.Image(\n",
    "                data[0], caption=\"Pred: {} Truth: {}\".format(classes[target_u[0].item()], classes[clean_targets[0]])))\n",
    "\n",
    "\n",
    "    print('====> Test1 set acc: {:.4f}'.format(top1.avg))\n",
    "    wandb.log({\n",
    "        \"Test Examples\": example_images,\n",
    "        \"top1.avg\": top1.avg})\n",
    "\n",
    "\n",
    "\n",
    "    return top1.avg,  top1.avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- etc. funtions --- #\n",
    "def save_generated_img(image, name, epoch, nrow=8):\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    if epoch % 5 == 0:\n",
    "        save_path = 'results/'+name+'_'+str(epoch)+'.png'\n",
    "        save_image(image, save_path, nrow=nrow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vae(\n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    batch_size=128, \n",
    "    epochs=100, \n",
    "    z_dim=2, \n",
    "    est_loader= None,\n",
    "    cls_model = None, \n",
    "    out_dir = \"\", \n",
    "    select_ratio =0.25, \n",
    "    pretrained = 0, \n",
    "    dataset=\"CIFAR10\",\n",
    "    noise_rate = 0.45\n",
    "    ):\n",
    "    vae_lr = 0.001\n",
    "    vae_args.lr = 0.001\n",
    "    vae_args.LOG_INTERVAL = 100\n",
    "    vae_args.BATCH_SIZE = batch_size\n",
    "    vae_args.EPOCHS = epochs\n",
    "    vae_args.z_dim = z_dim\n",
    "    vae_args.pretrained = pretrained\n",
    "    vae_args.dataset = dataset\n",
    "    vae_args.select_ratio = select_ratio\n",
    "    vae_args.epoch_decay_start = 1000\n",
    "    vae_args.noise_rate = noise_rate\n",
    "    vae_args.forget_rate = noise_rate\n",
    "    vae_args.exponent = 1\n",
    "    vae_args.num_gradual = 10\n",
    "    mom1 = 0.9\n",
    "    mom2 = 0.1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(vae_args)\n",
    "    best_acc = 0\n",
    "  \n",
    "    # os.makedirs(out_dir, exist_ok=True)conda install -c conda-forge --update-deps --force-reinstall ipykernel -y\n",
    "\n",
    "    vae_args.alpha_plan = [vae_args.lr] * vae_args.EPOCHS\n",
    "    vae_args.beta1_plan = [mom1] * vae_args.EPOCHS\n",
    "\n",
    "    for i in range(vae_args.epoch_decay_start, vae_args.EPOCHS):\n",
    "        vae_args.alpha_plan[i] = float(vae_args.EPOCHS - i) / (vae_args.EPOCHS - vae_args.epoch_decay_start) * vae_args.lr\n",
    "        vae_args.beta1_plan[i] = mom2\n",
    "\n",
    "    vae_args.rate_schedule = np.ones(vae_args.EPOCHS)*vae_args.forget_rate \n",
    "    vae_args.rate_schedule[:vae_args.num_gradual] = np.linspace(0, vae_args.forget_rate **vae_args.exponent, vae_args.num_gradual)\n",
    "    print( vae_args.rate_schedule)\n",
    "    # exit()\n",
    "\n",
    "    if dataset == \"CLOTH1M\":\n",
    "        vae_model1 = models.__dict__[\"VAE_\"+vae_args.dataset](z_dim=vae_args.z_dim, num_classes=14)\n",
    "        vae_model2 = models.__dict__[\"VAE_\"+vae_args.dataset](z_dim=vae_args.z_dim, num_classes=14)\n",
    "    else:\n",
    "        vae_model1 = models.__dict__[\"VAE_\"+vae_args.dataset](z_dim=vae_args.z_dim, num_classes=train_loader.dataset._get_num_classes())\n",
    "        vae_model2 = models.__dict__[\"VAE_\"+vae_args.dataset](z_dim=vae_args.z_dim, num_classes=train_loader.dataset._get_num_classes())\n",
    "\n",
    "    model = {\"vae_model1\":vae_model1.to(device), \"vae_model2\":vae_model2.to(device)}\n",
    "\n",
    "    optimizers = {'vae1':torch.optim.Adam(model[\"vae_model1\"].parameters(), lr=vae_args.lr),'vae2':torch.optim.Adam(model[\"vae_model2\"].parameters(), lr=vae_args.lr)}\n",
    "    test_acc = 0\n",
    "\n",
    "    wandb.watch(model[\"vae_model1\"], log=\"all\")\n",
    "    wandb.watch(model[\"vae_model2\"], log=\"all\")\n",
    "\n",
    "    for epoch in range(0, vae_args.EPOCHS):\n",
    "        adjust_learning_rate(optimizers['vae1'], epoch)\n",
    "       \n",
    "        adjust_learning_rate(optimizers['vae2'], epoch)\n",
    "        train(epoch, model, train_loader, optimizers, device)\n",
    "\n",
    "\n",
    "        curr_test_acc1, curr_test_acc2 = test(epoch, model, test_loader, device, vae_args.dataset)\n",
    "        # if vae_args.EPOCHS% 20 == 0:\n",
    "\n",
    "        if vae_args.EPOCHS-epoch<=10:\n",
    "            print(epoch)\n",
    "            test_acc += curr_test_acc1\n",
    "\n",
    "    test_acc = test_acc/10\n",
    "    save_model(state ={  'epoch': epoch + 1,'state_dict': vae_model1.state_dict(), 'avg_acc1': test_acc, 'last_acc1':curr_test_acc1 },out=out_dir)\n",
    "    print(\"vae avg acc1: \",test_acc)\n",
    "    print(\"vae last acc1: \",curr_test_acc1) \n",
    "    return test_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_dict = {\"FASHIONMNIST\":\"resnet18\",\"CIFAR10\":\"resnet34\",\"CIFAR100\":\"resnet50\",\"SVHN\":\"resnet34\"}\n",
    "input_channel_dict = {\"FASHIONMNIST\":1,\"CIFAR10\":3,\"CIFAR100\":3,\"SVHN\":3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 150\n",
    "z_dim = 25\n",
    "num_hidden_layers = 2\n",
    "flip_rate_fixed = 0.45\n",
    "train_frac = 1\n",
    "trainval_split = 1\n",
    "noise_type = 'instance'\n",
    "seed = 1\n",
    "dataset_ = \"cifar10\"\n",
    "dataset = \"CIFAR10\"\n",
    "select_ratio = 0\n",
    "pretrained = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "building dataset...\n",
      "<zip object at 0x7f93e82c0188>\n",
      "tensor([6, 9, 9,  ..., 9, 1, 1])\n",
      "0.1\n",
      "1\n",
      "50000\n",
      "split validation set from training data\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "base_dir = dataset+\"/\"+noise_type+str(flip_rate_fixed)+\"/\"+str(seed)+\"/\"\n",
    "\n",
    "\n",
    "if seed is not None:\n",
    "    fix_seed(seed)\n",
    "train_val_loader, train_loader, val_loader, est_loader, test_loader = load_noisydata(\n",
    "    dataset = dataset,  \n",
    "    noise_type = noise_type,\n",
    "    random_state = seed, \n",
    "    batch_size = batch_size, \n",
    "    add_noise = True, \n",
    "    flip_rate_fixed = flip_rate_fixed, \n",
    "    trainval_split = trainval_split,\n",
    "    train_frac = train_frac\n",
    ")\n",
    "test_dataset = test_loader.dataset\n",
    "train_dataset = train_loader.dataset\n",
    "\n",
    "\n",
    "print(train_loader.dataset._get_num_classes())\n",
    "\n",
    "\n",
    "out_dir = base_dir+\"co_teaching/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Only 108 Image will be uploaded.\n"
     ]
    }
   ],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "img, target, clean_target, hat_clean_target, confidenice = next(iter(train_loader))\n",
    "input_images = [wandb.Image(x, caption=f\"Noisy Label:{classes[y]}, Clean Label:{classes[z]}\") \n",
    "                           for x, y, z in zip(img, target, clean_target)]\n",
    "wandb.log({\"input/images\": input_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names of the columns in your Table\n",
    "column_names = [\"Images\", \"IDNL\", \"Clean\",\"Hat_Clean\", \"Confidence\"]\n",
    "img, target, clean_target, hat_clean_target, confidenice = next(iter(train_loader))\n",
    "# Prepare your data, row-wise\n",
    "# You can log filepaths or image tensors with wandb.Image\n",
    "input_images = [[wandb.Image(x), classes[y], classes[i],classes[h],c] \n",
    "                           for x, y, i,h,c in zip(img, target, clean_target, hat_clean_target, confidenice)]\n",
    "\n",
    "# Create your W&B Table\n",
    "val_table = wandb.Table(data=input_images, columns=column_names)\n",
    "\n",
    "# Log the Table to W&B\n",
    "wandb.log({'input/table': val_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++run co-teaching vae+++++++++++++++++\n",
      "namespace(BATCH_SIZE=128, EPOCHS=150, LOG_INTERVAL=100, dataset='CIFAR10', epoch_decay_start=1000, exponent=1, forget_rate=0.45, lr=0.001, noise_rate=0.45, num_gradual=10, pretrained=0, select_ratio=0.25, z_dim=25)\n",
      "[0.   0.05 0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45\n",
      " 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/idnl/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average loss: 0.02230/0.02159/0.42010/0.43715\n",
      "====> train noisy acc: 20.3966\n",
      "====> Test1 set acc: 27.6300\n",
      "====> Epoch: 1 Average loss: 0.01695/0.01705/0.21211/0.21476\n",
      "====> train noisy acc: 25.0841\n",
      "====> Test1 set acc: 33.8900\n",
      "====> Epoch: 2 Average loss: 0.01640/0.01628/0.20554/0.20657\n",
      "====> train noisy acc: 27.5300\n",
      "====> Test1 set acc: 30.9100\n",
      "====> Epoch: 3 Average loss: 0.01581/0.01588/0.20192/0.20304\n",
      "====> train noisy acc: 29.3650\n",
      "====> Test1 set acc: 34.9900\n",
      "====> Epoch: 4 Average loss: 0.01525/0.01526/0.19813/0.19836\n",
      "====> train noisy acc: 31.1538\n",
      "====> Test1 set acc: 45.9100\n",
      "====> Epoch: 5 Average loss: 0.01471/0.01456/0.19509/0.19438\n",
      "====> train noisy acc: 32.2997\n",
      "====> Test1 set acc: 34.7400\n",
      "====> Epoch: 6 Average loss: 0.01434/0.01406/0.19205/0.19029\n",
      "====> train noisy acc: 33.5938\n",
      "====> Test1 set acc: 46.0200\n",
      "====> Epoch: 7 Average loss: 0.01379/0.01351/0.18897/0.18645\n",
      "====> train noisy acc: 34.2889\n",
      "====> Test1 set acc: 45.7900\n",
      "====> Epoch: 8 Average loss: 0.01326/0.01301/0.18569/0.18374\n",
      "====> train noisy acc: 35.1763\n",
      "====> Test1 set acc: 52.3200\n",
      "====> Epoch: 9 Average loss: 0.01304/0.01269/0.18466/0.18116\n",
      "====> train noisy acc: 35.6510\n",
      "====> Test1 set acc: 45.9500\n",
      "====> Epoch: 10 Average loss: 0.01222/0.01193/0.17958/0.17729\n",
      "====> train noisy acc: 36.7248\n",
      "====> Test1 set acc: 53.1000\n",
      "====> Epoch: 11 Average loss: 0.01173/0.01139/0.18086/0.17493\n",
      "====> train noisy acc: 37.0132\n",
      "====> Test1 set acc: 47.1400\n",
      "====> Epoch: 12 Average loss: 0.01168/0.01137/0.17505/0.17187\n",
      "====> train noisy acc: 37.3397\n",
      "====> Test1 set acc: 55.1800\n",
      "====> Epoch: 13 Average loss: 0.01137/0.01125/0.17409/0.17029\n",
      "====> train noisy acc: 38.2812\n",
      "====> Test1 set acc: 47.0900\n",
      "====> Epoch: 14 Average loss: 0.01132/0.01092/0.17312/0.16799\n",
      "====> train noisy acc: 38.2913\n",
      "====> Test1 set acc: 54.6200\n",
      "====> Epoch: 15 Average loss: 0.01087/0.01058/0.17147/0.16658\n",
      "====> train noisy acc: 39.4591\n",
      "====> Test1 set acc: 55.9800\n",
      "====> Epoch: 16 Average loss: 0.01067/0.01041/0.17124/0.16465\n",
      "====> train noisy acc: 39.3810\n",
      "====> Test1 set acc: 59.0900\n",
      "====> Epoch: 17 Average loss: 0.01026/0.00998/0.16605/0.16336\n",
      "====> train noisy acc: 40.0561\n",
      "====> Test1 set acc: 61.1500\n",
      "====> Epoch: 18 Average loss: 0.01012/0.00991/0.16589/0.16268\n",
      "====> train noisy acc: 40.1102\n",
      "====> Test1 set acc: 64.8900\n",
      "====> Epoch: 19 Average loss: 0.00984/0.00951/0.16393/0.16046\n",
      "====> train noisy acc: 41.0617\n",
      "====> Test1 set acc: 67.1400\n",
      "====> Epoch: 20 Average loss: 0.00974/0.00969/0.16334/0.16027\n",
      "====> train noisy acc: 40.9916\n",
      "====> Test1 set acc: 64.7600\n",
      "====> Epoch: 21 Average loss: 0.00966/0.00934/0.16146/0.15748\n",
      "====> train noisy acc: 41.3902\n",
      "====> Test1 set acc: 63.3700\n",
      "====> Epoch: 22 Average loss: 0.00918/0.00903/0.15925/0.15712\n",
      "====> train noisy acc: 41.8970\n",
      "====> Test1 set acc: 70.1500\n",
      "====> Epoch: 23 Average loss: 0.00916/0.00893/0.15868/0.15584\n",
      "====> train noisy acc: 42.3918\n",
      "====> Test1 set acc: 63.4500\n",
      "====> Epoch: 24 Average loss: 0.00895/0.00881/0.15638/0.15495\n",
      "====> train noisy acc: 42.8085\n",
      "====> Test1 set acc: 55.5500\n",
      "====> Epoch: 25 Average loss: 0.00884/0.00877/0.15504/0.15337\n",
      "====> train noisy acc: 43.0469\n",
      "====> Test1 set acc: 65.3300\n",
      "====> Epoch: 26 Average loss: 0.00868/0.00845/0.15378/0.15224\n",
      "====> train noisy acc: 43.4696\n",
      "====> Test1 set acc: 62.0200\n",
      "====> Epoch: 27 Average loss: 0.00875/0.00838/0.15363/0.15139\n",
      "====> train noisy acc: 43.4836\n",
      "====> Test1 set acc: 71.4500\n",
      "====> Epoch: 28 Average loss: 0.00847/0.00832/0.15130/0.15049\n",
      "====> train noisy acc: 43.9002\n",
      "====> Test1 set acc: 71.1200\n",
      "====> Epoch: 29 Average loss: 0.00834/0.00820/0.15117/0.15039\n",
      "====> train noisy acc: 44.1046\n",
      "====> Test1 set acc: 71.4700\n",
      "====> Epoch: 30 Average loss: 0.00819/0.00808/0.14979/0.14897\n",
      "====> train noisy acc: 44.2348\n",
      "====> Test1 set acc: 73.6400\n",
      "====> Epoch: 31 Average loss: 0.00814/0.00794/0.14876/0.14796\n",
      "====> train noisy acc: 44.2328\n",
      "====> Test1 set acc: 73.0900\n",
      "====> Epoch: 32 Average loss: 0.00802/0.00885/0.14764/0.14998\n",
      "====> train noisy acc: 44.7316\n",
      "====> Test1 set acc: 74.5900\n",
      "====> Epoch: 33 Average loss: 0.00770/0.00784/0.14655/0.14728\n",
      "====> train noisy acc: 45.1402\n",
      "====> Test1 set acc: 71.0000\n",
      "====> Epoch: 34 Average loss: 0.00758/0.00754/0.14611/0.14522\n",
      "====> train noisy acc: 45.3165\n",
      "====> Test1 set acc: 72.6600\n",
      "====> Epoch: 35 Average loss: 0.00762/0.00743/0.14533/0.14522\n",
      "====> train noisy acc: 45.4928\n",
      "====> Test1 set acc: 73.7800\n",
      "====> Epoch: 36 Average loss: 0.00744/0.00734/0.14423/0.14392\n",
      "====> train noisy acc: 45.7893\n",
      "====> Test1 set acc: 66.9600\n",
      "====> Epoch: 37 Average loss: 0.00750/0.00740/0.14433/0.14356\n",
      "====> train noisy acc: 45.7372\n",
      "====> Test1 set acc: 73.2100\n",
      "====> Epoch: 38 Average loss: 0.00699/0.00699/0.14247/0.14238\n",
      "====> train noisy acc: 46.1919\n",
      "====> Test1 set acc: 71.7000\n",
      "====> Epoch: 39 Average loss: 0.00731/0.00714/0.14259/0.14226\n",
      "====> train noisy acc: 46.6046\n",
      "====> Test1 set acc: 71.3200\n",
      "====> Epoch: 40 Average loss: 0.00737/0.00719/0.14219/0.14156\n",
      "====> train noisy acc: 46.5024\n",
      "====> Test1 set acc: 74.8100\n",
      "====> Epoch: 41 Average loss: 0.00731/0.00731/0.14167/0.14147\n",
      "====> train noisy acc: 46.5505\n",
      "====> Test1 set acc: 76.5000\n",
      "====> Epoch: 42 Average loss: 0.00711/0.00718/0.14042/0.14093\n",
      "====> train noisy acc: 46.7708\n",
      "====> Test1 set acc: 71.8500\n",
      "====> Epoch: 43 Average loss: 0.00701/0.00690/0.14047/0.14002\n",
      "====> train noisy acc: 46.7628\n",
      "====> Test1 set acc: 77.0900\n",
      "====> Epoch: 44 Average loss: 0.00685/0.00674/0.13922/0.13912\n",
      "====> train noisy acc: 47.1274\n",
      "====> Test1 set acc: 77.4300\n",
      "====> Epoch: 45 Average loss: 0.00677/0.00669/0.13922/0.13896\n",
      "====> train noisy acc: 47.4539\n",
      "====> Test1 set acc: 77.5600\n",
      "====> Epoch: 46 Average loss: 0.00689/0.00685/0.13861/0.13837\n",
      "====> train noisy acc: 47.4960\n",
      "====> Test1 set acc: 77.1500\n",
      "====> Epoch: 47 Average loss: 0.00688/0.00669/0.13820/0.13789\n",
      "====> train noisy acc: 47.7284\n",
      "====> Test1 set acc: 74.5200\n",
      "====> Epoch: 48 Average loss: 0.03637/0.01614/0.20922/0.13919\n",
      "====> train noisy acc: 26.0677\n",
      "====> Test1 set acc: 36.3900\n",
      "====> Epoch: 49 Average loss: 0.02341/0.01256/0.19698/0.13781\n",
      "====> train noisy acc: 28.9683\n",
      "====> Test1 set acc: 47.9700\n",
      "====> Epoch: 50 Average loss: 0.01859/0.01034/0.18157/0.13661\n",
      "====> train noisy acc: 34.0505\n",
      "====> Test1 set acc: 61.2000\n",
      "====> Epoch: 51 Average loss: 0.01370/0.00892/0.16541/0.13668\n",
      "====> train noisy acc: 38.9643\n",
      "====> Test1 set acc: 67.6700\n",
      "====> Epoch: 52 Average loss: 0.01017/0.00754/0.15323/0.13619\n",
      "====> train noisy acc: 42.9748\n",
      "====> Test1 set acc: 72.9400\n",
      "====> Epoch: 53 Average loss: 0.00864/0.00688/0.14638/0.13498\n",
      "====> train noisy acc: 44.7977\n",
      "====> Test1 set acc: 75.0200\n",
      "====> Epoch: 54 Average loss: 0.00765/0.00653/0.14274/0.13527\n",
      "====> train noisy acc: 45.8614\n",
      "====> Test1 set acc: 77.4600\n",
      "====> Epoch: 55 Average loss: 0.00734/0.00641/0.14049/0.13443\n",
      "====> train noisy acc: 46.7067\n",
      "====> Test1 set acc: 77.0900\n",
      "====> Epoch: 56 Average loss: 0.00699/0.00624/0.13896/0.13380\n",
      "====> train noisy acc: 47.3197\n",
      "====> Test1 set acc: 76.8100\n",
      "====> Epoch: 57 Average loss: 0.00690/0.00616/0.13738/0.13334\n",
      "====> train noisy acc: 47.7444\n",
      "====> Test1 set acc: 76.6400\n",
      "====> Epoch: 58 Average loss: 0.00685/0.00620/0.13704/0.13289\n",
      "====> train noisy acc: 47.8365\n",
      "====> Test1 set acc: 77.4600\n",
      "====> Epoch: 59 Average loss: 0.00659/0.00595/0.13576/0.13196\n",
      "====> train noisy acc: 48.0429\n",
      "====> Test1 set acc: 76.5300\n",
      "====> Epoch: 60 Average loss: 0.00658/0.00598/0.13545/0.13180\n",
      "====> train noisy acc: 48.2031\n",
      "====> Test1 set acc: 76.9800\n",
      "====> Epoch: 61 Average loss: 0.00651/0.00605/0.13495/0.13204\n",
      "====> train noisy acc: 48.0970\n",
      "====> Test1 set acc: 77.8400\n",
      "====> Epoch: 62 Average loss: 0.00635/0.00596/0.13469/0.13123\n",
      "====> train noisy acc: 48.4996\n",
      "====> Test1 set acc: 77.9400\n",
      "====> Epoch: 63 Average loss: 0.00639/0.00592/0.13437/0.13066\n",
      "====> train noisy acc: 48.7500\n",
      "====> Test1 set acc: 78.8200\n",
      "====> Epoch: 64 Average loss: 0.00650/0.00588/0.13403/0.13037\n",
      "====> train noisy acc: 48.7800\n",
      "====> Test1 set acc: 76.5700\n",
      "====> Epoch: 65 Average loss: 0.00623/0.00579/0.13366/0.13043\n",
      "====> train noisy acc: 48.6398\n",
      "====> Test1 set acc: 78.4300\n",
      "====> Epoch: 66 Average loss: 0.00621/0.00580/0.13286/0.12982\n",
      "====> train noisy acc: 48.9583\n",
      "====> Test1 set acc: 78.6700\n",
      "====> Epoch: 67 Average loss: 0.00628/0.00570/0.13328/0.12959\n",
      "====> train noisy acc: 48.9483\n",
      "====> Test1 set acc: 78.5200\n",
      "====> Epoch: 68 Average loss: 0.00624/0.00562/0.13223/0.12928\n",
      "====> train noisy acc: 49.1627\n",
      "====> Test1 set acc: 78.9600\n",
      "====> Epoch: 69 Average loss: 0.00610/0.00562/0.13229/0.12892\n",
      "====> train noisy acc: 49.2388\n",
      "====> Test1 set acc: 76.3900\n",
      "====> Epoch: 70 Average loss: 0.00600/0.00557/0.13195/0.12840\n",
      "====> train noisy acc: 49.2528\n",
      "====> Test1 set acc: 77.8900\n",
      "====> Epoch: 71 Average loss: 0.00612/0.00568/0.13157/0.12797\n",
      "====> train noisy acc: 49.5092\n",
      "====> Test1 set acc: 74.9800\n",
      "====> Epoch: 72 Average loss: 0.00602/0.00555/0.13134/0.12743\n",
      "====> train noisy acc: 49.4010\n",
      "====> Test1 set acc: 75.1700\n",
      "====> Epoch: 73 Average loss: 0.00600/0.00547/0.13103/0.12728\n",
      "====> train noisy acc: 49.7756\n",
      "====> Test1 set acc: 78.0100\n",
      "====> Epoch: 74 Average loss: 0.00594/0.00543/0.12972/0.12670\n",
      "====> train noisy acc: 49.8518\n",
      "====> Test1 set acc: 78.6200\n",
      "====> Epoch: 75 Average loss: 0.00578/0.00539/0.12946/0.12706\n",
      "====> train noisy acc: 49.8618\n",
      "====> Test1 set acc: 77.1400\n",
      "====> Epoch: 76 Average loss: 0.00595/0.00553/0.12926/0.12622\n",
      "====> train noisy acc: 50.0040\n",
      "====> Test1 set acc: 74.8400\n",
      "====> Epoch: 77 Average loss: 0.00582/0.00535/0.12967/0.12569\n",
      "====> train noisy acc: 49.8918\n",
      "====> Test1 set acc: 77.5000\n",
      "====> Epoch: 78 Average loss: 0.00575/0.00537/0.12909/0.12580\n",
      "====> train noisy acc: 50.1002\n",
      "====> Test1 set acc: 78.8100\n",
      "====> Epoch: 79 Average loss: 0.00579/0.00539/0.12847/0.12583\n",
      "====> train noisy acc: 49.9980\n",
      "====> Test1 set acc: 79.8500\n",
      "====> Epoch: 80 Average loss: 0.00574/0.00525/0.12845/0.12485\n",
      "====> train noisy acc: 50.2925\n",
      "====> Test1 set acc: 77.1400\n",
      "====> Epoch: 81 Average loss: 0.00591/0.00508/0.12828/0.12442\n",
      "====> train noisy acc: 50.3546\n",
      "====> Test1 set acc: 78.3500\n",
      "====> Epoch: 82 Average loss: 0.00566/0.00532/0.12778/0.12469\n",
      "====> train noisy acc: 50.6591\n",
      "====> Test1 set acc: 78.6600\n",
      "====> Epoch: 83 Average loss: 0.00555/0.00520/0.12698/0.12392\n",
      "====> train noisy acc: 50.8654\n",
      "====> Test1 set acc: 77.1100\n",
      "====> Epoch: 84 Average loss: 0.00559/0.00504/0.12736/0.12351\n",
      "====> train noisy acc: 50.4547\n",
      "====> Test1 set acc: 78.2700\n",
      "====> Epoch: 85 Average loss: 0.00553/0.00517/0.12607/0.12338\n",
      "====> train noisy acc: 50.8313\n",
      "====> Test1 set acc: 80.1800\n",
      "====> Epoch: 86 Average loss: 0.00544/0.00505/0.12586/0.12303\n",
      "====> train noisy acc: 50.8413\n",
      "====> Test1 set acc: 79.3300\n",
      "====> Epoch: 87 Average loss: 0.00540/0.00502/0.12580/0.12277\n",
      "====> train noisy acc: 50.9235\n",
      "====> Test1 set acc: 78.7100\n",
      "====> Epoch: 88 Average loss: 0.00540/0.00513/0.12515/0.12262\n",
      "====> train noisy acc: 51.0677\n",
      "====> Test1 set acc: 79.2100\n",
      "====> Epoch: 89 Average loss: 0.00548/0.00514/0.12500/0.12213\n",
      "====> train noisy acc: 51.2159\n",
      "====> Test1 set acc: 80.2900\n",
      "====> Epoch: 90 Average loss: 0.00531/0.00497/0.12407/0.12200\n",
      "====> train noisy acc: 51.4804\n",
      "====> Test1 set acc: 78.8900\n",
      "====> Epoch: 91 Average loss: 0.00524/0.00481/0.12411/0.12161\n",
      "====> train noisy acc: 51.4663\n",
      "====> Test1 set acc: 78.9600\n",
      "====> Epoch: 92 Average loss: 0.00546/0.00498/0.12470/0.12119\n",
      "====> train noisy acc: 51.4924\n",
      "====> Test1 set acc: 78.4500\n",
      "====> Epoch: 93 Average loss: 0.00529/0.00498/0.12366/0.12120\n",
      "====> train noisy acc: 51.5625\n",
      "====> Test1 set acc: 79.2800\n",
      "====> Epoch: 94 Average loss: 0.00527/0.00493/0.12344/0.12080\n",
      "====> train noisy acc: 51.6947\n",
      "====> Test1 set acc: 74.0400\n",
      "====> Epoch: 95 Average loss: 0.00515/0.00471/0.12289/0.12004\n",
      "====> train noisy acc: 51.7488\n",
      "====> Test1 set acc: 79.0700\n",
      "====> Epoch: 96 Average loss: 0.00523/0.00481/0.12303/0.12016\n",
      "====> train noisy acc: 51.7829\n",
      "====> Test1 set acc: 78.6000\n",
      "====> Epoch: 97 Average loss: 0.00510/0.00464/0.12225/0.11949\n",
      "====> train noisy acc: 52.0653\n",
      "====> Test1 set acc: 79.6200\n",
      "====> Epoch: 98 Average loss: 0.00536/0.00476/0.12230/0.11948\n",
      "====> train noisy acc: 52.0913\n",
      "====> Test1 set acc: 79.7300\n",
      "====> Epoch: 99 Average loss: 0.00506/0.00484/0.12154/0.11949\n",
      "====> train noisy acc: 52.1875\n",
      "====> Test1 set acc: 79.0200\n",
      "====> Epoch: 100 Average loss: 0.00490/0.00471/0.12116/0.11924\n",
      "====> train noisy acc: 52.5100\n",
      "====> Test1 set acc: 80.1100\n",
      "====> Epoch: 101 Average loss: 0.00503/0.00478/0.12103/0.11938\n",
      "====> train noisy acc: 52.3618\n",
      "====> Test1 set acc: 76.7500\n",
      "====> Epoch: 102 Average loss: 0.00503/0.00485/0.12075/0.11898\n",
      "====> train noisy acc: 52.5160\n",
      "====> Test1 set acc: 76.2200\n",
      "====> Epoch: 103 Average loss: 0.00513/0.00471/0.12124/0.11860\n",
      "====> train noisy acc: 52.2576\n",
      "====> Test1 set acc: 78.0000\n",
      "====> Epoch: 104 Average loss: 0.00506/0.00462/0.12072/0.11796\n",
      "====> train noisy acc: 52.5280\n",
      "====> Test1 set acc: 79.3500\n",
      "====> Epoch: 105 Average loss: 0.00491/0.00465/0.11963/0.11762\n",
      "====> train noisy acc: 52.9046\n",
      "====> Test1 set acc: 79.6700\n",
      "====> Epoch: 106 Average loss: 0.00487/0.00451/0.11984/0.11733\n",
      "====> train noisy acc: 52.7925\n",
      "====> Test1 set acc: 79.5900\n",
      "====> Epoch: 107 Average loss: 0.00485/0.00447/0.11983/0.11713\n",
      "====> train noisy acc: 52.8365\n",
      "====> Test1 set acc: 78.3200\n",
      "====> Epoch: 108 Average loss: 0.00509/0.00466/0.11970/0.11756\n",
      "====> train noisy acc: 52.7304\n",
      "====> Test1 set acc: 78.6900\n",
      "====> Epoch: 109 Average loss: 0.00479/0.00460/0.11901/0.11692\n",
      "====> train noisy acc: 53.0509\n",
      "====> Test1 set acc: 78.0700\n",
      "====> Epoch: 110 Average loss: 0.00500/0.00460/0.11893/0.11649\n",
      "====> train noisy acc: 53.0429\n",
      "====> Test1 set acc: 79.9800\n",
      "====> Epoch: 111 Average loss: 0.00473/0.00464/0.11787/0.11667\n",
      "====> train noisy acc: 53.1951\n",
      "====> Test1 set acc: 79.5000\n",
      "====> Epoch: 112 Average loss: 0.00480/0.00448/0.11862/0.11603\n",
      "====> train noisy acc: 52.9908\n",
      "====> Test1 set acc: 79.7600\n",
      "====> Epoch: 113 Average loss: 0.00497/0.00449/0.11888/0.11577\n",
      "====> train noisy acc: 53.0769\n",
      "====> Test1 set acc: 79.9300\n",
      "====> Epoch: 114 Average loss: 0.00475/0.00460/0.11779/0.11623\n",
      "====> train noisy acc: 53.3754\n",
      "====> Test1 set acc: 80.7500\n",
      "====> Epoch: 115 Average loss: 0.00475/0.00455/0.11795/0.11589\n",
      "====> train noisy acc: 53.4776\n",
      "====> Test1 set acc: 78.8300\n",
      "====> Epoch: 116 Average loss: 0.00460/0.00436/0.11720/0.11531\n",
      "====> train noisy acc: 53.6699\n",
      "====> Test1 set acc: 79.0100\n",
      "====> Epoch: 117 Average loss: 0.00465/0.00438/0.11684/0.11428\n",
      "====> train noisy acc: 53.8321\n",
      "====> Test1 set acc: 79.8800\n",
      "====> Epoch: 118 Average loss: 0.00454/0.00443/0.11697/0.11492\n",
      "====> train noisy acc: 53.8522\n",
      "====> Test1 set acc: 79.1100\n",
      "====> Epoch: 119 Average loss: 0.00459/0.00438/0.11651/0.11453\n",
      "====> train noisy acc: 53.8622\n",
      "====> Test1 set acc: 78.8200\n",
      "====> Epoch: 120 Average loss: 0.00458/0.00425/0.11616/0.11413\n",
      "====> train noisy acc: 53.9143\n",
      "====> Test1 set acc: 79.4500\n",
      "====> Epoch: 121 Average loss: 0.00466/0.00442/0.11608/0.11381\n",
      "====> train noisy acc: 53.8562\n",
      "====> Test1 set acc: 78.8100\n",
      "====> Epoch: 122 Average loss: 0.00472/0.00428/0.11599/0.11328\n",
      "====> train noisy acc: 53.8762\n",
      "====> Test1 set acc: 78.9300\n",
      "====> Epoch: 123 Average loss: 0.00464/0.00438/0.11563/0.11348\n",
      "====> train noisy acc: 54.0865\n",
      "====> Test1 set acc: 78.2700\n",
      "====> Epoch: 124 Average loss: 0.00450/0.00419/0.11529/0.11338\n",
      "====> train noisy acc: 54.3169\n",
      "====> Test1 set acc: 79.3300\n",
      "====> Epoch: 125 Average loss: 0.00437/0.00419/0.11493/0.11319\n",
      "====> train noisy acc: 54.1526\n",
      "====> Test1 set acc: 79.9700\n",
      "====> Epoch: 126 Average loss: 0.00448/0.00419/0.11474/0.11252\n",
      "====> train noisy acc: 54.4291\n",
      "====> Test1 set acc: 78.0200\n",
      "====> Epoch: 127 Average loss: 0.00456/0.00418/0.11437/0.11252\n",
      "====> train noisy acc: 54.3870\n",
      "====> Test1 set acc: 78.8700\n",
      "====> Epoch: 128 Average loss: 0.00442/0.00414/0.11432/0.11233\n",
      "====> train noisy acc: 54.4171\n",
      "====> Test1 set acc: 79.7300\n",
      "====> Epoch: 129 Average loss: 0.00448/0.00423/0.11390/0.11184\n",
      "====> train noisy acc: 54.6214\n",
      "====> Test1 set acc: 78.1500\n",
      "====> Epoch: 130 Average loss: 0.00630/0.00475/0.12023/0.11241\n",
      "====> train noisy acc: 52.8566\n",
      "====> Test1 set acc: 78.8700\n",
      "====> Epoch: 131 Average loss: 0.00449/0.00432/0.11393/0.11273\n",
      "====> train noisy acc: 54.5573\n",
      "====> Test1 set acc: 79.2000\n",
      "====> Epoch: 132 Average loss: 0.00427/0.00400/0.11313/0.11144\n",
      "====> train noisy acc: 54.8558\n",
      "====> Test1 set acc: 78.3600\n",
      "====> Epoch: 133 Average loss: 0.00423/0.00402/0.11293/0.11104\n",
      "====> train noisy acc: 55.0621\n",
      "====> Test1 set acc: 79.1200\n",
      "====> Epoch: 134 Average loss: 0.00424/0.00406/0.11254/0.11111\n",
      "====> train noisy acc: 55.0341\n",
      "====> Test1 set acc: 76.5100\n",
      "====> Epoch: 135 Average loss: 0.00447/0.00406/0.11274/0.11052\n",
      "====> train noisy acc: 55.0200\n",
      "====> Test1 set acc: 78.8900\n",
      "====> Epoch: 136 Average loss: 0.00434/0.00414/0.11217/0.11104\n",
      "====> train noisy acc: 55.2504\n",
      "====> Test1 set acc: 77.8900\n",
      "====> Epoch: 137 Average loss: 0.00416/0.00400/0.11153/0.11035\n",
      "====> train noisy acc: 55.4507\n",
      "====> Test1 set acc: 79.8300\n",
      "====> Epoch: 138 Average loss: 0.00427/0.00402/0.11151/0.11025\n",
      "====> train noisy acc: 55.2764\n",
      "====> Test1 set acc: 79.1000\n",
      "====> Epoch: 139 Average loss: 0.00422/0.00404/0.11161/0.10994\n",
      "====> train noisy acc: 55.1322\n",
      "====> Test1 set acc: 79.3400\n",
      "====> Epoch: 140 Average loss: 0.00425/0.00410/0.11129/0.11028\n",
      "====> train noisy acc: 55.4026\n",
      "====> Test1 set acc: 79.8000\n",
      "140\n",
      "====> Epoch: 141 Average loss: 0.00418/0.00392/0.11120/0.10948\n",
      "====> train noisy acc: 55.4547\n",
      "====> Test1 set acc: 77.8000\n",
      "141\n",
      "====> Epoch: 142 Average loss: 0.00421/0.00391/0.11089/0.10916\n",
      "====> train noisy acc: 55.7512\n",
      "====> Test1 set acc: 77.9900\n",
      "142\n",
      "====> Epoch: 143 Average loss: 0.00416/0.00409/0.11066/0.10896\n",
      "====> train noisy acc: 55.6510\n",
      "====> Test1 set acc: 76.5300\n",
      "143\n",
      "====> Epoch: 144 Average loss: 0.00414/0.00408/0.11030/0.10910\n",
      "====> train noisy acc: 55.9936\n",
      "====> Test1 set acc: 79.1900\n",
      "144\n",
      "====> Epoch: 145 Average loss: 0.00427/0.00401/0.11014/0.10843\n",
      "====> train noisy acc: 55.7532\n",
      "====> Test1 set acc: 78.0900\n",
      "145\n",
      "====> Epoch: 146 Average loss: 0.00453/0.00424/0.11089/0.10855\n",
      "====> train noisy acc: 55.5789\n",
      "====> Test1 set acc: 72.5700\n",
      "146\n",
      "====> Epoch: 147 Average loss: 0.00456/0.00403/0.11127/0.10797\n",
      "====> train noisy acc: 55.5349\n",
      "====> Test1 set acc: 78.8500\n",
      "147\n",
      "====> Epoch: 148 Average loss: 0.00422/0.00399/0.10978/0.10788\n",
      "====> train noisy acc: 55.7993\n",
      "====> Test1 set acc: 79.1100\n",
      "148\n",
      "====> Epoch: 149 Average loss: 0.00413/0.00396/0.10894/0.10780\n",
      "====> train noisy acc: 56.2981\n",
      "====> Test1 set acc: 77.9100\n",
      "149\n",
      "vae avg acc1:  77.784\n",
      "vae last acc1:  77.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77.784, {'vae_model1': VAE_CIFAR10(\n",
       "    (y_encoder): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "    )\n",
       "    (z_encoder): CONV_Encoder_CIFAR(\n",
       "      (embed_class): Linear(in_features=10, out_features=1024, bias=True)\n",
       "      (embed_data): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (encoder): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (fc_mu): Linear(in_features=1024, out_features=25, bias=True)\n",
       "      (fc_logvar): Linear(in_features=1024, out_features=25, bias=True)\n",
       "    )\n",
       "    (x_decoder): CONV_Decoder_CIFAR(\n",
       "      (decoder_input): Linear(in_features=35, out_features=1024, bias=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (final_layer): Sequential(\n",
       "        (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (t_decoder): CONV_T_Decoder(\n",
       "      (embed_class): Linear(in_features=10, out_features=1024, bias=True)\n",
       "      (embed_data): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (encoder): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       "    )\n",
       "  ), 'vae_model2': VAE_CIFAR10(\n",
       "    (y_encoder): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "    )\n",
       "    (z_encoder): CONV_Encoder_CIFAR(\n",
       "      (embed_class): Linear(in_features=10, out_features=1024, bias=True)\n",
       "      (embed_data): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (encoder): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (fc_mu): Linear(in_features=1024, out_features=25, bias=True)\n",
       "      (fc_logvar): Linear(in_features=1024, out_features=25, bias=True)\n",
       "    )\n",
       "    (x_decoder): CONV_Decoder_CIFAR(\n",
       "      (decoder_input): Linear(in_features=35, out_features=1024, bias=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (final_layer): Sequential(\n",
       "        (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (t_decoder): CONV_T_Decoder(\n",
       "      (embed_class): Linear(in_features=10, out_features=1024, bias=True)\n",
       "      (embed_data): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (encoder): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       "    )\n",
       "  )})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"++++++++++++run co-teaching vae+++++++++++++++++\")\n",
    "run_vae(\n",
    "    train_loader = train_loader, \n",
    "    est_loader=est_loader, \n",
    "    test_loader= test_loader, \n",
    "    batch_size=batch_size, \n",
    "    epochs= epochs,\n",
    "    z_dim= z_dim,\n",
    "    cls_model = None,\n",
    "    noise_rate=0.45\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "685a60fb40084e5f249e9198c9c3b8d4c952afdb9f94d2d375a80013aec9b163"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('idnl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
